
from CarbonFootPrint import  CodeCarbon
from extensions.GCS_Bucket import upload_to_gcs
import service
import sys


def get_script_description_prompt() -> str:
        return """
    
    You are an expert Terraform infrastructure generator. 
Write a single, minimal, production-ready Terraform configuration in pure HCL based on the following variable schema and example values.

Rules:
- Use only HCL syntax (no Markdown, no comments, no explanations).
- Use the variable names and types provided below.
- Make sure all resources are logically connected (e.g., subnets reference the VPC).
- Output only valid Terraform code.

Variable Schema:

1. VPC
   - cidr_block (string): "10.0.0.0/16"
   - enable_dns_support (bool): true
   - enable_dns_hostnames (bool): true

2. Subnet
   - vpc_id (string): Reference to VPC
   - cidr_block (string): "10.0.1.0/24"
   - availability_zone (string): "us-west-2a"

3. Security Group
   - vpc_id (string): Reference to VPC
   - ingress (list(object)):
     ingress = [{
       from_port = 5432
       to_port   = 5432
       protocol  = "tcp"
       cidr_blocks = ["10.0.0.0/16"]
     }]

Instructions:
- Use these inputs to build a working Terraform configuration that:
  - Creates a VPC
  - Adds a subnet in the VPC
  - Creates a security group attached to that VPC
- Output must be valid `.tf` code only.
"""

    

def get_script_comparison_prompt(script1, script2) -> str:
    prompt = f"""Copmare these two terrafrom script, and tell me if they are logically correct/identical.\n 
    Script1: {script1}\n 
    Script2: {script2}\n
    Answer your question with a percentage, on how much it is identical. Identical percentage"""

    return prompt

def print_output(llm_output,template_output, emissions, elapsed, usage, comparison, project_name):
    print("-------------------------------------------------------------------------------------------------")
    print(f'        This is Script Generation and Carbon Emission Calculation using {project_name}'          )
    print("-------------------------------------------------------------------------------------------------")
    print("-----This is output generated by llm/slm----\n")
    print(llm_output)
    print("------This is output template on github-----\n")
    print(template_output)
    print("")
    print("--------------This is emission--------------\n")
    print(emissions)
    print("")
    print("------------This is elapsed Time------------\n")
    print(elapsed)
    print("")
    print("----------------This is usage---------------\n")
    print(usage)
    print("")
    print("-------------------------------------------------------------------------------------------------")
    print("             Script Quality, by comparing it with template using llm")
    print("-------------------------------------------------------------------------------------------------")
    print(comparison)
    

def generate_result(choice, script_template, name_script):
     # Map choices to your service callables (all must be defined in service)

    initial_prompt = get_script_description_prompt()

    call_options = {
        "1": service.get_OPENAI_terraform_script,
        "2": service.get_GEMINI_terraform_script,
        "3": service.get_StarCoder2_terraform_script,
        "4": service.get_CLAUDE_terraform_script,
        "5": service.get_QWEN_terraform_script,
        "6": service.get_GEMMA_terraform_script,
        
    }

    project_name_options = {
        "1": "OpenAI",
        "2": "Gemini",
        "3": "Star Coder 2",
        "4": "Claude",
        "5": "Qwen",
        "6": "Gemma",
    }

    _model_call = call_options.get(choice)
    _project_name = project_name_options.get(choice)
    if _model_call is None:
        print("Invalid choice. Please run again and choose 1–6.")
        sys.exit(1)
    
    """
    tracking_mode:     "machine" measure the power consumptions of the entire machine (default)
                       "process" try and isolate the tracked processes in isolation
    """

    output, emissions, elapsed , usage = CodeCarbon.run_with_local_emissions(
        prompt=initial_prompt,
        model_call=_model_call,  
        tracking_mode="process",
        project_name= _project_name,
        output_dir="Output",
    )
    comparison_prompt = get_script_comparison_prompt(script_template, output)
    comparison = service.compare_scripts_using_open_Ai(comparison_prompt)

    return output, emissions, elapsed, usage, comparison, _project_name
    
    


    




if __name__ == "__main__":
    
    name_script = "Real Test232"

    script_template ="""

    module "cluster" {
  source  = "terraform-aws-modules/rds-aurora/aws"

  name           = "test-aurora-db-postgres96"
  engine         = "aurora-postgresql"
  engine_version = "14.5"
  instance_class = "db.r6g.large"
  instances = {
    one = {}
    2 = {
      instance_class = "db.r6g.2xlarge"
    }
  }

  vpc_id               = "vpc-12345678"
  db_subnet_group_name = "db-subnet-group"
  security_group_rules = {
    ex1_ingress = {
      cidr_blocks = ["10.20.0.0/20"]
    }
    ex1_ingress = {
      source_security_group_id = "sg-12345678"
    }
  }

  storage_encrypted   = true
  apply_immediately   = true
  monitoring_interval = 10

  enabled_cloudwatch_logs_exports = ["postgresql"]

  tags = {
    Environment = "dev"
    Terraform   = "true"
  }
}
    """

    choice = input(
        "\nChoose LLM:\n\n"
        "1. openai\n2. gemini\n3. StarCoder 2\n4. claude\n\nor SLM:\n\n"
        "5. qwen\n6. gemma\nChoice: "
    ).strip()

    output, emissions, elapsed, usage, comparison, _project_name = generate_result(choice, script_template, name_script)

    print_output(output, script_template, emissions, elapsed, usage, comparison, _project_name)
    upload_to_gcs(_project_name, name_script, output, script_template, comparison)
"""""“Give me a list of all the essential Terraform input variables, provider arguments, and resource arguments I should define if I want to generate a Terraform configuration for [RESOURCE / GOAL] (e.g., a VPC on AWS). Include their names, data types, and example values.”"""